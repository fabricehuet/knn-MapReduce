\subsection{Load Balance}
In a MapReduce job, the completion time of the Map phase and the Reduce phase depends on the longest-running task. This makes load balancing particularly 
important. In this sense, the partitioning strategy should ensure that the processing time of each task is roughly the same, so as to achieve the optimal 
overall 
performance. In our context, each task is used to calculate the pairwise distance of $<r_i, s_i>$ in the partition $<R_i, S_i>$. Thus, the number of distances that needs to be 
calculated is $\left| R_i \right|$ $\times$ $\left| S_i \right|$. If all tasks have to compute 
the same number of distances, they should have a roughly equivalent duration.  Hence, the optimal partition strategy should make:
\begin{center}
 $\forall$ i $\neq$ j, $\left| R_i \right|$ $\times$ $\left| S_i \right|$ = $\left| R_j \right|$ $\times$ $\left| S_j \right|$.
\end{center}

This is hard to ensure in practice. It is however possible
to obtain a sub-optimal solution in the following situation. Since 
$S_i$ is the set of possible nearest neighbors for the 
elements in $R_i$, then:
\begin{center}
$\forall$ i $\neq$ j, 

if $\left| R_i \right|$ = $\left| R_j \right|$ or $\left| S_i \right|$ = $\left| S_j \right|$, 

then $\left| R_i \right|$ $\times$ $\left| S_i \right|$ $\approx$ $\left| R_j \right|$ $\times$ $\left| S_j \right|$
\end{center} 
That is to say, if the number of objects in each partition of $R$ is equivalent, then the sum of the number of $k$ nearest neighbors of all objects in each partition can be considered approximately equivalent, and vice versa.

So an efficient partitioning should try to enforce either (1) $\left| R_i \right|$ = $\left| R_j \right|$ or (2) $\left| S_i \right|$ = $\left| S_j 
\right|$.
%When using space filling curves, the distance from the optimal solution (the true kNN) is often bounded, as explained in \cite{liao2001high}.

In \cite{Zhang:2012:EPK:2247596.2247602}, the authors give a short proof which 
shows that the worst-case complexity for (1) is equal to:
\begin{equation}
O\left(\left|R_i\right| \times log\left|S_i\right|\right) = O\left(\frac{\left|R\right|}{n} \times log\left|S\right|\right) \label{eq:complexity1}
\end{equation}


and for choice (2), the worst-case complexity is equal to:
\begin{equation}
O\left(\left|R_i\right| \times log\left|S_i\right|\right) = O\left(\left|R\right|\times log\frac{\left|S\right|}{n}\right) \label{eq:complexity2}
\end{equation}
where $n$ is the number of partitions. Since $n$~$\ll \left|S\right|$, the 
optimal partitioning is achieved when $\left| R_i \right|$ = $\left| R_j \right|$.


\subsection{Accuracy}
%Among the systems we have introduced, one crucial property that we have not 
%discussed yet is whether the system produces a reliable answer or not. In 
%order to quickly get answers, some systems prefer to loosen the accuracy of the 
%kNN computation. That is, in an approximate kNN computation, the results are 
%the same as in an exact kNN computation with a high probability. Nevertheless, results can be sub-optimal in some cases and even 
%incorrect with a low probability. This can be acceptable in situations where responsiveness is crucial, for example when there are user interactions. In 
%some other cases, only an exact 
%kNN computation is appropriate. Hence it is an important criteria, depending on the use cases.  

Usually, the lack of accuracy is the direct consequence of techniques such as  $z$-values and LSH used in the pre-processing step. In 
\cite{Zhang:2012:EPK:2247596.2247602} (H-zkNNJ), the authors show that when the dimension of the data increases, the quality of 
the results tends to decrease. This can be counterbalanced by increasing the number of random shifts applied to the data, thereby 
increasing the size of the resulting dataset. Experiments show that three 
shifts of the initial dataset (in dimension 15) are sufficient to achieve a good approximation 
(less than 10\% of errors shown in the experiences), while controlling the computation time. A similar result can be obtained with LSH by 
increasing the number of hash functions used. However, when using space filling curves, the distance from the optimal solution (the true kNN) is often bounded, which is not the case for LSH, as explained in \cite{liao2001high}.

%a consequence of a pre-processing step 
%performed to make the whole computation faster.
%
% Indeed, when dimensionality 
%increases, the distance between the nearest and the furthest neighbors tends to 
%decrease \cite{Beyer:1999:NNM:645503.656271}. This so called  
%\emph{curse of dimensionality} strongly influences the efficiency of tree-based exact kNN methods. 
%%\TODO{How? and why are we talking about centralized 
%%approaches here?}
%Also, dealing with data in high dimension greatly increases the complexity of the 
%partitioning and the computation of the distances. Many techniques such as 
%Voronoi-based diagrams or k-means, are in 
%theory applicable but become increasingly inefficient as the number of 
%dimensions increases. 
%
%Thus, one widely used method consists in reducing each point in the 
%data space into a point in a single dimension, while preserving the distance 
%relation between points. The representative techniques are $z$-values and LSH, as seen in the preprocessing section \ref{data_preprocessing}. 
%However this step cannot be performed without loss of precision, and 
%consequently the whole remaining process of the kNN computation suffers from 
%this reduction, possibly leading to incorrect results.
%
%In \cite{Zhang:2012:EPK:2247596.2247602} (H-zkNNJ), the authors reduce the dimension of 
%their data into one dimension, and are then able to partition 
%the data into intervals that contain the desired number of nearest neighbors. 
%By doing so, the authors gain one order of magnitude on the time needed 
%to compute approximate kNN compared to the most efficient exact solution they 
%found, and the gap tends to grow as the number of dimensions of the data 
%increases. However, when the dimension of the data increases, the quality of 
%the results tends to decrease, which they are able to counteract by 
%increasing the number of different shifts used to reduce the dimension of the 
%data to one dimension. The authors show in their experimentation that three 
%shifts of the initial dataset are sufficient to achieve a good approximation 
%(less than 10\% of errors shown in the experiences), while controlling the computation time.
%%\TODO{This 
%%is the z-value paper?Give more qualitative details maybe? do they 
%%theoretically estimate the error? What about the LSH paper? any result?}
%
%LSH is also one of the projection method aimed to transform data from high dimension to low dimension. The performance of LSH depends on parameter tuning. Like z-value method, we also need to process projection many times to increase the accuracy. But unlike z-value who generates some shifts of the original data, LSH will perform directly on the original data but with many hash functions.

%Although approximate, these solutions 
%are indeed fully usable in practice and can provide a high level of confidence, depending on the quality of results required by the application. 
%Nonetheless, error bounds are difficult nay impossible to compute, and mostly experiments enable to find the best parameters for a given approximate computation.
%\TODO{we need a better conclusion. Is there a bound or the error depends on the dataset? Is the gain always an order of magnitude? what is the cost of shifting?}
